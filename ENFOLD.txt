ES Python dependencies
----------------------

Current version of this package requires elasticsearch_dsl. It is
necessary to add the 'elasticsearch-dsl' egg to the buildout eggs.
Alternatively, it can be added to the eggs in the celery part. 
Run the buildout again to get that dependency on existing installations.

ES configuration on zope.conf
-----------------------------

The elasticsearch directive now supports the following keys:

- max_blobsize. Max length of files to index, in bytes. If a file is
  larger than this size, it will not be indexed, and this will be logged.
  Default is zero, which means index everything.

- request_timeout. The default connection timeout is 10 seconds. Using this
  key it can be set to any number of seconds.

- use_celery. If true, indexing will be done in async celery tasks. This
  requires that celery is correctly configured.

- indexed_chars. Maximum number of characters to extract from attachments.
  Default is 100,000. Use -1 for infinite.

Example:

zope-conf-additional =
    %import collective.es.index
    <elasticsearch>
    query 127.0.0.1:92000
    ingest 127.0.0.1:92000
    request_timeout 20
    max_blobsize 10000000 # 10 MB
    indexed_chars 200000
    use_celery true
    </elasticsearch>

It is necessary to add this configuration to the buildout and rerun it
whenever a change is made to these parameters.

Celery configuration
--------------------

NOTE: The configuration now uses collective.celery, so it has changed.

The collective.celery package requires adding the celery and
collective.celery eggs to the mian buildout section eggs. Example:

eggs = 
    celery
    Plone
    elasticsearch
    elasticsearch-dsl
    collective.es.index
    collective.celery

We still use the celery-broker part, for clarity. The celery part is
still required, but is simpler:

[celery-broker]
host = 127.0.0.1
port = 6379

[celery]
recipe = zc.recipe.egg
environment-vars = ${buildout:environment-vars}
eggs = 
    ${buildout:eggs}
    flower
scripts = pcelery flower

The celery part depends on having some variables added to the main
environment-vars section:

environment-vars =
    CELERY_BROKER_URL redis://${celery-broker:host}:${celery-broker:port}
    CELERY_RESULT_BACKEND redis://${celery-broker:host}:${celery-broker:port}
    CELERY_TASKS collective.es.index.tasks

Additional Zope configuration
-----------------------------

There's now a hook in collective.celery for carrying out additional zope
configuration before running the tasks. If the tasks module contains an
'extra_config' method, it is passed the zope startup object at worker
initialization time. This is used by collective.es.index to run the
elasticsearch configuration method.

Monitoring celery tasks
-----------------------

Celery needs to be started as an independent process. It's recommended to
use supervisord for this. To try it out from the command line, you can run
"bin/pcelery worker" from the buildout directory. Note that the script is
now named 'pcelery' and it needs a path to the zope configuration. Example:

$ bin/pcelery worker parts/client1/etc/zope.conf

Flower is included in this setup. Run "bin/flower" from the buildout
directory and consult the dashboard at http://localhost:5555 using a
browser. Note that the broker is now a requried parameter:

$ bin/flower --broker redis://127.0.0.1:6379

Removing b64 attribute
----------------------

To get the b64 attribute removal working on an existing elasticsearch
install, it's necessary to clear the old ingest pipeline, so that 
collective.es.index can install the new one. To do this, you can use a
python prompt, like this:

>>> from elasticsearch import Elasticsearch
>>> es = Elasticsearch()
>>> es.ingest.delete_pipeline('attachment_ingest_plone_plone') 

Highlight support
-----------------

For every search result, a list of highlights from extracted text is
saved as a dictionary in the current request annotations. The
dictionary is keyed by object UID.

To get the annotations from Python code:

from collective.es.index.esproxyindex import HIGHLIGHT_KEY
from zope.annotation.interfaces import IAnnotations
annotations = IAnnotations(REQUEST)
highlights = annotations[HIGHLIGHT_KEY]
obj_highlights = highlights[OBJ_UID]
highlight_text = '<br/>'.join(obj_highlights)

Highlights are just lists of HTML text fragments with the query term
enclosed in <em> tags.

Collective.auditlog
-------------------

Collective.fingerpointing has been replaced by collective.auditlog. Make
sure to change the eggs in the buildout to reflect this.

For now, collective.auditlog uses SQLAlchemy for storing data. To use
postgres, it's necessary to add the 'pyscopg2' egg to the buildout. Once
the product is installed, add the correct connection URL to the product
setup. Example:

postgresql://cguardia:cguardia@localhost/auditlog

By default, collective.auditlog uses content rules to define which events
to capture. An additional mechanism has been added that allows the site to
automatically log the various events supported by collective.auditlog.
Simply choose from the picklist in the config for this to work. If no
events are selected, no logging will occur.

It is possible to log custom events from application code by using the
AuditableActionPerformedEvent, like this:

from zope event import notify
from collective.auditlog.interfaces import AuditableActionPerformedEvent
notify(AuditableActionPerformedEvent(obj, request, "action", "note"))

'obj', refers to the affected content object; 'request' is the current zope
request, 'action' and 'note' correspond to the logged action and its
description, respectively. All parameters are required, but everything
except obj can be set to None if no value is available. 

In addition to control panel configuration, connection parameters can be
set using the zope-conf-additional directive in the buildout. Note that
this will take precedence over any control panel configuration. Example:

zope-conf-additional =
    <product-config collective.auditlog>
        audit-connection-string postgres://cguardia:cguardia@localhost/auditlog
        audit-connection-params {"pool_recycle": 3600, "echo": true}
    </product-config>

There is now a view for the audit log entries, located at @@auditlog-view.
There is no link to it from the control panel at the moment. The view uses
infinite scrollong rather than pagination for looking at the logs.
